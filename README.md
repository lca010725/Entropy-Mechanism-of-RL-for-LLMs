<div align="center">

  <h2><b> Entropy Mechanism of RL for LLMs </b></h2>
  <h4> An overview of research in Entropy Mechanism of Reinforcement Learning for Large Language Models</h4>

</div>


</div>

## ðŸ“œContent

## ðŸ‘€ Introduction
---

## Keywords Convention

![](https://img.shields.io/badge/Clip--Higher-blue) Main Features

![](https://img.shields.io/badge/AAAI2026-orange) Conference

![](https://img.shields.io/badge/Reconstruction-lightgray) Abbreviation

## ðŸš€ Papers

### Token Constraint Strategies

- [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476) [[code](https://dapo-sia.github.io/)] ![](https://img.shields.io/badge/abs-2025.03-orange) ![](https://img.shields.io/badge/Clip--Higher-blue) ![](https://img.shields.io/badge/DAPO-lightgray)
- [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617) [[code](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL)] ![](https://img.shields.io/badge/abs-2025.05-orange) ![](https://img.shields.io/badge/Clip--Cov_&_KL--Cov-blue)
- [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939) [[code](https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr)] ![](https://img.shields.io/badge/NIPS2025-orange)
- [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585) [[code](https://github.com/MiniMax-AI/MiniMax-M1)] ![](https://img.shields.io/badge/abs-2025.06-orange) ![](https://img.shields.io/badge/CISPO-lightgray)
- [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758) ![](https://img.shields.io/badge/AAAI2026-orange)
- [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778) [[code](https://github.com/wizard-III/ArcherCodeR)] ![](https://img.shields.io/badge/abs-2025.07-orange)
- [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265) [[code](https://empgseed-seed.github.io/)] ![](https://img.shields.io/badge/abs-2025.09-orange) ![](https://img.shields.io/badge/EMPG-lightgray)
- [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927) [[code](https://github.com/WooooDyy/BAPO)] ![](https://img.shields.io/badge/abs-2025.10-orange) ![](https://img.shields.io/badge/BAPO-lightgray)
- [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248) [[code](https://github.com/yk7333/EntroPIC)] ![](https://img.shields.io/badge/abs-2025.11-orange)


### KL/Entropy Loss Strategies

- [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312) [[code](https://github.com/SkyworkAI/Skywork-OR1)] ![](https://img.shields.io/badge/abs-2025.05-orange) ![](https://img.shields.io/badge/Adaptive_Entropy_Control-blue)
- [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864) [[code](https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B)] ![](https://img.shields.io/badge/abs-2025.05-orange) ![](https://img.shields.io/badge/ProRL-lightgray)


### Positive & Negative Sample 
- [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347) [[code](https://github.com/TianHongZXY/RLVR-Decomposed)] ![](https://img.shields.io/badge/NIPS2025-orange) ![](https://img.shields.io/badge/PSR_&_NSR-lightgray)
